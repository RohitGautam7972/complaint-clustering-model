"""
Complaint Clubbing Pipeline
Single-file prototype implementing the roadmap you provided.

Features included:
- CSV loader and synthetic data generator
- Text preprocessing (basic)
- Sentence-BERT embeddings (all-MiniLM-L6-v2)
- Pairwise cosine similarity
- Keyword overlap (Jaccard on top-N tokens)
- Combined final_score = weighted sum
- Simple dynamic threshold function
- Incremental-style custom clustering (visited set)
- Postprocessing: merge small clusters, split loose clusters
- Confidence score, representative selection, keyword extraction (yake)
- Simple evaluation: silhouette score (if >1 cluster) and purity
- FastAPI endpoint scaffold to serve clustering

Notes:
- This is a prototype designed for clarity and experimentation, not for massive scale.
- Install required packages:
    pip install sentence-transformers scikit-learn numpy pandas yake fastapi uvicorn nltk
- NLTK: you'll need stopwords/wordnet if lemmatization is enabled. Run:
    import nltk; nltk.download('stopwords'); nltk.download('punkt')

Usage example (CLI):
    python complaint_clubbing_pipeline.py --csv complaints.csv

"""

import argparse
import itertools
import json
import os
import random
import re
import string
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score

# Optional imports that might not be available in all environments
try:
    from sentence_transformers import SentenceTransformer
except Exception as e:
    SentenceTransformer = None

try:
    import yake
except Exception:
    yake = None

# Basic text preprocessing utilities
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

STOPWORDS = set()
try:
    nltk.data.find('corpora/stopwords')
    STOPWORDS = set(stopwords.words('english'))
except Exception:
    try:
        nltk.download('stopwords')
        nltk.download('punkt')
        STOPWORDS = set(stopwords.words('english'))
    except Exception:
        STOPWORDS = set()

PUNCT_TABLE = str.maketrans('', '', string.punctuation)


def preprocess_text(text: str, lowercase: bool = True, remove_stopwords: bool = True) -> str:
    if not isinstance(text, str):
        text = str(text)
    if lowercase:
        text = text.lower()
    # remove urls
    text = re.sub(r'http\S+|www\S+', ' ', text)
    # remove punctuation
    text = text.translate(PUNCT_TABLE)
    # tokenize
    tokens = word_tokenize(text)
    if remove_stopwords and STOPWORDS:
        tokens = [t for t in tokens if t not in STOPWORDS]
    return ' '.join(tokens)


# Synthetic data generator (simple)
MESS_ISSUES = [
    "Mess food is stale and cold",
    "Food quality poor, found hair in food",
    "Mess serving small portions",
    "Food is oily and causes stomach ache",
]
WIFI_ISSUES = [
    "Internet not working in hostel",
    "WiFi disconnects every 10 minutes",
    "Slow internet speed in evening",
]
PLUMBING = [
    "No water in washroom",
    "Water leakage in block A",
    "Tap broken in bathroom",
]


def generate_synthetic(n=300, start_date: str = '2025-08-01') -> pd.DataFrame:
    rows = []
    start = datetime.fromisoformat(start_date)
    categories = [('Mess', MESS_ISSUES), ('Internet', WIFI_ISSUES), ('Plumbing', PLUMBING)]
    for i in range(n):
        cat, pool = random.choice(categories)
        text = random.choice(pool)
        # add small variation
        if random.random() < 0.3:
            text += ' ' + random.choice(['please fix', 'urgent', 'since yesterday', 'today morning'])
        ts = start + timedelta(days=random.randint(0, 60), hours=random.randint(0, 23))
        rows.append({'id': i + 1, 'complaint_text': text, 'category': cat, 'timestamp': ts.isoformat()})
    return pd.DataFrame(rows)


# Embedding helper
def load_model(model_name='all-MiniLM-L6-v2'):
    if SentenceTransformer is None:
        raise ImportError('sentence-transformers not installed. pip install sentence-transformers')
    return SentenceTransformer(model_name)


def compute_embeddings(model, texts: List[str], batch_size: int = 64) -> np.ndarray:
    return model.encode(texts, convert_to_numpy=True, batch_size=batch_size, show_progress_bar=True)


# Keyword overlap (Jaccard between top-n tokens)
def top_tokens(text: str, n: int = 5) -> List[str]:
    tokens = text.split()
    # simple frequency ranking
    freq = {}
    for t in tokens:
        freq[t] = freq.get(t, 0) + 1
    top = sorted(freq.keys(), key=lambda x: -freq[x])[:n]
    return top


def jaccard(a: List[str], b: List[str]) -> float:
    if not a or not b:
        return 0.0
    A, B = set(a), set(b)
    inter = len(A & B)
    union = len(A | B)
    return inter / union if union > 0 else 0.0


# Final score combining multiple signals
def final_score(i: int, j: int, sim_matrix: np.ndarray, preproc_texts: List[str], categories: List[str], weights=(0.7, 0.2, 0.1)) -> float:
    cosine = float(sim_matrix[i, j])
    kw_overlap = jaccard(top_tokens(preproc_texts[i]), top_tokens(preproc_texts[j]))
    cat_match = 1.0 if categories[i] == categories[j] else 0.0
    w_text, w_kw, w_cat = weights
    return w_text * cosine + w_kw * kw_overlap + w_cat * cat_match


# Dynamic threshold function (simple heuristic):
# base threshold + adjustment based on cluster density or recency if desired
def dynamic_threshold(i: int, j: int, sim_matrix: np.ndarray, base=0.65) -> float:
    # we can make threshold tighter for high-similarity pairs to avoid drift
    pair_sim = sim_matrix[i, j]
    # if pair sim is already high, allow grouping with slightly lower final score
    if pair_sim > 0.8:
        return base - 0.05
    return base


# Clustering algorithm: incremental / connected components-like
def custom_cluster_complaints(preproc_texts: List[str], embeddings: np.ndarray, categories: List[str], timestamps: List[str],
                              base_threshold: float = 0.65, merge_threshold: float = 0.8, min_cluster_size: int = 2) -> List[Dict[str, Any]]:
    n = len(preproc_texts)
    sim_matrix = cosine_similarity(embeddings)
    visited = set()
    clusters = []

    for i in range(n):
        if i in visited:
            continue
        current = [i]
        visited.add(i)
        for j in range(n):
            if j in visited:
                continue
            score = final_score(i, j, sim_matrix, preproc_texts, categories)
            threshold = dynamic_threshold(i, j, sim_matrix, base=base_threshold)
            # time window constraint example: only cluster if within 30 days
            try:
                dt_i = datetime.fromisoformat(timestamps[i])
                dt_j = datetime.fromisoformat(timestamps[j])
                time_ok = abs((dt_i - dt_j).days) <= 30
            except Exception:
                time_ok = True

            if score > threshold and time_ok:
                current.append(j)
                visited.add(j)
        clusters.append(current)

    # Postprocessing: merge clusters that are close
    clusters = merge_close_clusters(clusters, embeddings, merge_threshold)
    # Filter small clusters if required (optionally merge with nearest)
    clusters = filter_and_merge_small(clusters, embeddings, min_cluster_size)

    # Build output objects
    out = []
    for c in clusters:
        rep = choose_representative(c, preproc_texts, timestamps)
        conf = cluster_confidence(c, cosine_similarity(embeddings)) if len(c) > 1 else 1.0
        keywords = extract_keywords([preproc_texts[idx] for idx in c])
        out.append({'indices': c, 'representative_index': rep, 'confidence': conf, 'keywords': keywords})
    return out


# Merge close clusters by comparing cluster centroids or representatives
def merge_close_clusters(clusters: List[List[int]], embeddings: np.ndarray, merge_threshold: float) -> List[List[int]]:
    merged = []
    used = [False] * len(clusters)
    centroids = [np.mean(embeddings[c], axis=0) for c in clusters]
    for i, c in enumerate(clusters):
        if used[i]:
            continue
        merged_cluster = set(c)
        used[i] = True
        for j in range(i + 1, len(clusters)):
            if used[j]:
                continue
            sim = cosine_similarity(centroids[i].reshape(1, -1), centroids[j].reshape(1, -1))[0, 0]
            if sim > merge_threshold:
                merged_cluster.update(clusters[j])
                used[j] = True
        merged.append(sorted(list(merged_cluster)))
    return merged


# Filter and attempt to merge very small clusters into nearest cluster
def filter_and_merge_small(clusters: List[List[int]], embeddings: np.ndarray, min_cluster_size: int) -> List[List[int]]:
    if min_cluster_size <= 1:
        return clusters
    large = [c for c in clusters if len(c) >= min_cluster_size]
    small = [c for c in clusters if len(c) < min_cluster_size]
    for sc in small:
        # find nearest large cluster by centroid similarity
        best_idx = None
        best_sim = -1
        sc_cent = np.mean(embeddings[sc], axis=0)
        for idx, lc in enumerate(large):
            lc_cent = np.mean(embeddings[lc], axis=0)
            sim = cosine_similarity(sc_cent.reshape(1, -1), lc_cent.reshape(1, -1))[0, 0]
            if sim > best_sim:
                best_sim = sim
                best_idx = idx
        if best_idx is not None:
            large[best_idx].extend(sc)
        else:
            large.append(sc)
    return [sorted(c) for c in large]


def cluster_confidence(cluster: List[int], sim_matrix: np.ndarray) -> float:
    if len(cluster) < 2:
        return 1.0
    pairs = list(itertools.combinations(cluster, 2))
    values = [sim_matrix[i, j] for i, j in pairs]
    return float(np.mean(values))


def choose_representative(cluster: List[int], preproc_texts: List[str], timestamps: List[str]) -> int:
    # choose longest text; tie-breaker: most recent
    best = cluster[0]
    for idx in cluster:
        if len(preproc_texts[idx]) > len(preproc_texts[best]):
            best = idx
        elif len(preproc_texts[idx]) == len(preproc_texts[best]):
            try:
                if datetime.fromisoformat(timestamps[idx]) > datetime.fromisoformat(timestamps[best]):
                    best = idx
            except Exception:
                pass
    return best


# Keyword extraction wrapper using yake if available, otherwise fallback to token freq

def extract_keywords(texts: List[str], max_kw: int = 5) -> List[str]:
    if yake is not None:
        kw_extractor = yake.KeywordExtractor(lan='en', n=1, top=max_kw)
        joined = ' '.join(texts)
        kw = [k for k, score in kw_extractor.extract_keywords(joined)]
        return kw[:max_kw]
    # fallback
    tokens = ' '.join(texts).split()
    freq = {}
    for t in tokens:
        freq[t] = freq.get(t, 0) + 1
    top = sorted(freq.keys(), key=lambda x: -freq[x])[:max_kw]
    return top


# Simple purity metric given ground truth categories per complaint
# purity = sum(max_count_in_cluster) / N

def compute_purity(clusters: List[List[int]], ground_truth: List[str]) -> float:
    N = len(ground_truth)
    total = 0
    for c in clusters:
        counts = {}
        for idx in c:
            cat = ground_truth[idx]
            counts[cat] = counts.get(cat, 0) + 1
        if counts:
            total += max(counts.values())
    return total / N if N > 0 else 0.0


# FastAPI scaffold
FASTAPI_APP_SNIPPET = '''
from fastapi import FastAPI
from pydantic import BaseModel
from typing import List

app = FastAPI()

class Complaint(BaseModel):
    id: int
    complaint_text: str
    category: str = ''
    timestamp: str = ''

@app.post('/cluster')
def cluster_endpoint(complaints: List[Complaint]):
    # Implement using your pipeline's functions
    # Example: preprocess -> embeddings -> custom_cluster_complaints
    return {'detail': 'This is a scaffold. Integrate pipeline functions here.'}
'''


# CLI and example pipeline execution

def run_pipeline(csv_path: str = None, synth: bool = False, n_synth: int = 300):
    if csv_path is None and not synth:
        raise ValueError('Provide csv_path or synth=True')
    if synth:
        df = generate_synthetic(n_synth)
    else:
        df = pd.read_csv(csv_path)

    # Ensure columns exist
    required = ['id', 'complaint_text', 'category', 'timestamp']
    for c in required:
        if c not in df.columns:
            raise ValueError(f'Missing column: {c}')

    # Preprocess
    df['preproc'] = df['complaint_text'].apply(preprocess_text)
    texts = df['preproc'].tolist()
    categories = df['category'].astype(str).tolist()
    timestamps = df['timestamp'].astype(str).tolist()

    # Embeddings
    print('Loading embedding model...')
    model = load_model()
    embeddings = compute_embeddings(model, texts)

    # Clustering
    print('Running custom clustering...')
    clusters = custom_cluster_complaints(texts, embeddings, categories, timestamps)

    # Evaluation
    cluster_indices = [c['indices'] for c in clusters]
    # flatten labels for silhouette: assign cluster id per sample
    labels = np.zeros(len(texts), dtype=int) - 1
    for cid, c in enumerate(cluster_indices):
        for idx in c:
            labels[idx] = cid
    assigned = labels >= 0
    if len(set(labels[assigned])) > 1:
        sil = silhouette_score(embeddings[assigned], labels[assigned])
    else:
        sil = None
    purity = compute_purity(cluster_indices, categories)
    print(f'Found {len(clusters)} clusters. Silhouette: {sil} Purity: {purity}')
    # Pretty output
    results = []
    for i, c in enumerate(clusters):
        rep_idx = c['representative_index']
        results.append({'cluster_id': i, 'size': len(c['indices']), 'rep_text': texts[rep_idx], 'confidence': c['confidence'], 'keywords': c['keywords']})
    print(json.dumps(results, indent=2))
    return df, clusters


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--csv', type=str, default=None)
    parser.add_argument('--synth', action='store_true')
    parser.add_argument('--n', type=int, default=300)
    args = parser.parse_args()

    if args.synth:
        run_pipeline(synth=True, n_synth=args.n)
    else:
        run_pipeline(csv_path=args.csv)

